import streamlit as st
from neo4j import GraphDatabase
from neo4j_graphrag.retrievers import Text2CypherRetriever
from neo4j_graphrag.llm import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import Ollama
import pandas as pd
import re
import ast
import os
import json
from Bio import Entrez
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import asyncio

# ========== Streamlit UI ==========
st.set_page_config(page_title="Medical RAG System", layout="wide")
st.title("ğŸ§  Multi-Agent Medical RAG System")
st.markdown("çµåˆ **GraphRAG (PrimeKG)** + **PubMed Papers RAG**ï¼Œç‚ºæ‚¨æä¾›é†«å­¸ç ”ç©¶ç­”æ¡ˆ")

query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„é†«å­¸å•é¡Œ")

managing_model = st.selectbox(
    "é¸æ“‡ Managing Agent æ¨¡å‹",
    ["llama3", "mistral", "qwen2:7b", "gpt-oss"]
)
cypher_model = st.selectbox(
    "é¸æ“‡ Cypher æŸ¥è©¢æ¨¡å‹",
    ["llama3", "mistral", "gemma:7b", "qwen2:7b", "gpt-oss"]
)
answer_model = st.selectbox(
    "é¸æ“‡ å›ç­”ç”Ÿæˆæ¨¡å‹",
    ["llama3", "mistral", "gpt-oss"]
)

submit = st.button("æŸ¥è©¢")

# ========== Neo4j Initialization ==========
@st.cache_resource
def init_graph(cypher_model_name, answer_model_name):
    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))
    cypher_llm = OllamaLLM(model_name=cypher_model_name, model_params={"temperature": 0}, host="http://localhost:11434")
    answer_llm = Ollama(model=answer_model_name, temperature=0, base_url="http://localhost:11434")
    retriever = Text2CypherRetriever(driver=driver, llm=cypher_llm)
    return driver, cypher_llm, answer_llm, retriever

driver, cypher_llm, answer_llm, retriever = init_graph(cypher_model, answer_model)

# ========== PubMed Utility Functions ==========
Entrez.email = "youremail@gmail.com"  # TODO: Replace with your email

def search_pubmed(keyword, max_results=20):
    handle = Entrez.esearch(db="pubmed", term=keyword, retmax=max_results, sort="relevance")
    record = Entrez.read(handle)
    return record["IdList"]

def get_pmc_id(pmid):
    handle = Entrez.elink(dbfrom="pubmed", db="pmc", id=pmid)
    record = Entrez.read(handle)
    try:
        return record[0]['LinkSetDb'][0]['Link'][0]['Id']
    except:
        return None

def get_title_from_pubmed(pmid):
    handle = Entrez.efetch(db="pubmed", id=pmid, rettype="medline", retmode="text")
    lines = handle.read().split("\n")
    title_lines = [line[6:].strip() for line in lines if line.startswith("TI  ") or line.startswith("      ")]
    return " ".join(title_lines) if title_lines else ""

def get_fulltext_from_europe_pmc(pmcid):
    url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/PMC{pmcid}/fullTextXML"
    response = requests.get(url)
    return response.text if response.status_code == 200 else None

def extract_fulltext_structure(xml_text):
    soup = BeautifulSoup(xml_text, "xml")
    title = soup.find("article-title")
    abstract = soup.find("abstract")
    body_paragraphs = [p.get_text().strip() for p in soup.find_all("p") if p.get_text().strip()]
    return {
        "title": title.get_text().strip() if title else "",
        "abstract": abstract.get_text(separator=" ", strip=True) if abstract else "",
        "body": body_paragraphs
    }

def retrieve_papers(keyword, max_results=20):
    pmids = search_pubmed(keyword, max_results=max_results)
    papers = []
    for pmid in pmids:
        pmc_id = get_pmc_id(pmid)
        title = get_title_from_pubmed(pmid)
        entry = {
            "pmid": pmid,
            "title": title,
            "pubmed_url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
        }
        if pmc_id:
            xml = get_fulltext_from_europe_pmc(pmc_id)
            if xml:
                entry["fulltext"] = extract_fulltext_structure(xml)
        papers.append(entry)
    return papers

import re

# ç°¡å–®åœç”¨å­—ï¼Œå¯è‡ªè¡Œæ“´å……
_STOPWORDS = set("""
the a an and or of for to with without in on by from about as is are was were be being been into over under than then this that those these such using use used based among between
""".split())

def _clean_phrase(s: str) -> str:
    s = s.strip().strip('"').strip("'")
    s = re.sub(r"\s+", " ", s)
    return s

def derive_core_terms(user_query: str) -> list[str]:
    """
    å¾ä½¿ç”¨è€… query å‹•æ…‹æŠ½ä¸»é¡Œè©ï¼ˆç–¾ç—… / ä¸»é«”ï¼‰ï¼Œä½œç‚º must terms èˆ‡ PubMed æŸ¥è©¢æ ¸å¿ƒã€‚
    è¦å‰‡ï¼š
    - å…ˆæŠ“å¼•è™Ÿä¸­çš„ç‰‡èª
    - æ²’æœ‰å¼•è™Ÿå‰‡å–é•·åº¦ >= 3 çš„é—œéµè©ï¼Œå»åœç”¨å­—
    - è‡³å°‘ä¿ç•™ä¸€å€‹è©ï¼ˆæ•´å¥ï¼‰
    """
    q = user_query.strip()
    quoted = re.findall(r'"([^"]+)"|\'([^\']+)\'', q)
    phrases = [p[0] or p[1] for p in quoted]
    if phrases:
        core = [_clean_phrase(p) for p in phrases if _clean_phrase(p)]
    else:
        toks = [t for t in re.split(r"[^A-Za-z0-9\-]+", q) if t]
        toks = [t for t in toks if len(t) >= 3 and t.lower() not in _STOPWORDS]
        core = toks[:5] if toks else [q]
    # å»é‡ä¸¦ä¿ç•™åŸé †åº
    seen, core_unique = set(), []
    for t in core:
        tl = t.lower()
        if tl not in seen:
            seen.add(tl)
            core_unique.append(t)
    return core_unique

def contains_any(text: str, terms: list[str]) -> bool:
    t = text.lower()
    return any(term.lower() in t for term in terms)

# ========== PapersRAG RAG Helpers  ==========

# å¯é¸å¥—ä»¶ï¼šè‹¥å·²å®‰è£æœƒå•Ÿç”¨ï¼›æœªå®‰è£å‰‡è‡ªå‹•é™ç´š
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    _emb_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    _reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
except Exception:
    _emb_model = None
    _reranker = None

try:
    from rank_bm25 import BM25Okapi
except Exception:
    BM25Okapi = None

import math
import unicodedata

def _normalize_ws(s: str) -> str:
    return " ".join(s.split())

def _chunk_text(text: str, max_chars: int = 1800, overlap: int = 200):
    """
    ç°¡å–®ç”¨å­—å…ƒé•·åº¦åˆ‡å¡Šï¼ˆä¿å®ˆä¼°ç´„ 1.2â€“1.6 chars/tokenï¼‰ã€‚
    """
    text = _normalize_ws(text)
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks

def build_chunks_from_entry(entry: dict):
    """
    æŠŠå–®ç¯‡è«–æ–‡ entry è½‰ç‚º RAG å¯ç”¨çš„ chunksã€‚
    ä¾†æºï¼š
      - fulltext.abstract
      - fulltext.body (æ¯æ®µè½)
      - ç„¡å…¨æ–‡æ™‚ï¼Œè‡³å°‘ç”¨ title ç•¶ç·šç´¢
    """
    chunks = []
    pmid = entry.get("pmid")
    title = entry.get("title", "")
    url = entry.get("pubmed_url", "")

    # æ‘˜è¦
    if entry.get("fulltext", {}).get("abstract"):
        for i, ch in enumerate(_chunk_text(entry["fulltext"]["abstract"])):
            chunks.append({
                "pmid": pmid, "title": title, "where": "abstract",
                "chunk_id": f"abs-{i}", "text": ch, "pubmed_url": url
            })

    # æ­£æ–‡
    if entry.get("fulltext", {}).get("body"):
        for j, para in enumerate(entry["fulltext"]["body"]):
            para = _normalize_ws(para)
            if not para:
                continue
            # ä½ åŸæœ¬æ²’æœ‰ section åç¨±ï¼Œé€™è£¡ç›´æ¥æ¨™ body
            for i, ch in enumerate(_chunk_text(para)):
                chunks.append({
                    "pmid": pmid, "title": title, "where": "body",
                    "chunk_id": f"p{j}-{i}", "text": ch, "pubmed_url": url
                })

    # æ²’æœ‰å…¨æ–‡æ™‚ï¼Œè‡³å°‘ç”¨ title
    if not chunks:
        if title.strip():
            chunks.append({
                "pmid": pmid, "title": title, "where": "title",
                "chunk_id": "t-0", "text": title.strip(), "pubmed_url": url
            })

    return chunks

def _tokenize_for_bm25(txt: str):
    # å¾ˆé¬†çš„è‹±æ–‡ token åŒ–ï¼›ä¸­æ–‡ä»¥å­—å…ƒåˆ‡
    txt = unicodedata.normalize("NFKC", txt.lower())
    # ç•™ä¸‹å­—æ¯æ•¸å­—èˆ‡ç©ºç™½
    cleaned = []
    for ch in txt:
        if ch.isalnum() or ch.isspace():
            cleaned.append(ch)
        else:
            # ä¸­æ–‡ / å…¶ä»–èªç³»ï¼šç›´æ¥è¦–ç‚º token
            if ord(ch) > 127 and not ch.isspace():
                cleaned.append(f" {ch} ")
    return " ".join("".join(cleaned).split()).split()

class PapersMiniIndex:
    """
    éæŒä¹…åŒ–çš„å°å‹ç´¢å¼•ã€‚
    - è‹¥è£äº† sentence-transformersï¼šç”¨å‘é‡ç›¸ä¼¼åº¦
    - è‹¥è£äº† rank-bm25ï¼šç”¨ BM25
    - å…©è€…éƒ½æœ‰ï¼šåšç°¡å–® hybridï¼ˆåˆ†æ•¸æ¨™æº–åŒ–å¾Œåˆä½µï¼‰
    - å…¨éƒ½æ²’æœ‰ï¼šç”¨é—œéµè©å‘½ä¸­åˆ†æ•¸
    """
    def __init__(self, chunks: list[dict]):
        self.docs = [c["text"] for c in chunks]
        self.metas = chunks
        self._dense = None
        self._bm25 = None

        # Dense embeddingsï¼ˆå¯é¸ï¼‰
        if _emb_model is not None and self.docs:
            self._dense = _emb_model.encode(self.docs, show_progress_bar=False, normalize_embeddings=True)

        # BM25ï¼ˆå¯é¸ï¼‰
        if BM25Okapi is not None and self.docs:
            self._bm25 = BM25Okapi([_tokenize_for_bm25(d) for d in self.docs])

    def search(self, query: str, top_k_dense=10, top_k_bm25=10):
        scores = {}

        # Dense ç›¸ä¼¼åº¦
        if self._dense is not None:
            q = _emb_model.encode([query], show_progress_bar=False, normalize_embeddings=True)[0]
            # é¤˜å¼¦ç›¸ä¼¼åº¦
            import numpy as np
            dense_scores = np.dot(self._dense, q)
            # å– top_k_dense
            idxs = dense_scores.argsort()[-top_k_dense:][::-1]
            ds = dense_scores[idxs]
            # normalize åˆ° 0..1
            if len(ds) > 1:
                ds_norm = (ds - ds.min()) / (ds.max() - ds.min() + 1e-9)
            else:
                ds_norm = ds
            for i, s in zip(idxs, ds_norm):
                scores[i] = max(scores.get(i, 0.0), float(s))

        # BM25 åˆ†æ•¸
        if self._bm25 is not None:
            bm25 = self._bm25.get_scores(_tokenize_for_bm25(query))
            import numpy as np
            idxs = bm25.argsort()[-top_k_bm25:][::-1]
            bs = bm25[idxs]
            if len(bs) > 1:
                bs_norm = (bs - bs.min()) / (bs.max() - bs.min() + 1e-9)
            else:
                bs_norm = bs
            for i, s in zip(idxs, bs_norm):
                # å–æœ€å¤§ï¼ˆdense / bm25ï¼‰ä½œç‚º hybrid ç°¡å–®èåˆ
                scores[i] = max(scores.get(i, 0.0), float(s))

        # å¦‚æœå…©è€…éƒ½æ²’æœ‰ï¼Œå°±ç”¨é—œéµè©å‘½ä¸­æ•¸
        if not scores:
            q_terms = set(query.lower().split())
            for i, d in enumerate(self.docs):
                hit = sum(1 for t in q_terms if t in d.lower())
                if hit:
                    scores[i] = float(hit)

        # æŒ‰åˆ†æ•¸æ’åº
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        hits = [{"text": self.docs[i], "meta": self.metas[i], "score": s} for i, s in ranked]
        return hits

def rerank_hits(query: str, hits: list[dict], top_k: int = 8):
    """
    å¯é¸é‡æ’ï¼šè‹¥æœ‰ CrossEncoder å°±ç”¨ï¼›å¦å‰‡ç›´æ¥å–å‰ top_kã€‚
    """
    if not hits:
        return []
    if _reranker is None:
        return hits[:top_k]
    pairs = [(query, h["text"]) for h in hits[: max(32, top_k*2)]]
    scores = _reranker.predict(pairs)
    for h, s in zip(hits[:len(pairs)], scores):
        h["rerank"] = float(s)
    hits = sorted(hits[:len(pairs)], key=lambda x: x.get("rerank", 0.0), reverse=True)[:top_k]
    return hits

def make_cited_context(hits: list[dict]) -> str:
    """
    æŠŠå‘½ä¸­ç‰‡æ®µè½‰ç‚º LLM å¯åƒçš„å¸¶ç·¨è™Ÿå¼•æ–‡å€å¡Šã€‚
    """
    blocks = []
    for i, h in enumerate(hits, 1):
        m = h["meta"]
        pmid = m.get("pmid", "NA")
        title = m.get("title", "")
        where = m.get("where", "")
        url = m.get("pubmed_url", "")
        txt = h["text"]
        blocks.append(
            f"[{i}] PMID {pmid} | {title} | {where}\n{txt}\nLink: {url}"
        )
    return "\n\n".join(blocks)

def filter_chunks_by_terms(chunks: list[dict], must_terms: list[str]) -> list[dict]:
    """åœ¨åˆ‡å¡Šå¾Œç«‹å³å…ˆéæ¿¾ä¸€æ¬¡ï¼ˆé™ä½å¾ŒçºŒå™ªéŸ³ï¼‰ã€‚"""
    if not must_terms:
        return chunks
    out = []
    for c in chunks:
        if contains_any(c.get("text", ""), must_terms) or contains_any(c.get("title",""), must_terms):
            out.append(c)
    return out

def filter_hits_by_terms(hits: list[dict], must_terms: list[str]) -> list[dict]:
    """åœ¨æª¢ç´¢èˆ‡é‡æ’å¾Œå†éæ¿¾ä¸€æ¬¡ï¼Œç¢ºä¿æœ€å¾Œé€² LLM çš„ç‰‡æ®µéƒ½å«ä¸»é¡Œè©ã€‚"""
    if not must_terms:
        return hits
    out = []
    for h in hits:
        if contains_any(h.get("text",""), must_terms) or contains_any(h.get("meta",{}).get("title",""), must_terms):
            out.append(h)
    return out
 
# ========== Final Answer Synthesizer ==========
def synthesize_answer(query, graph_context, papers_context, model):
    llm = Ollama(model=model, temperature=0, base_url="http://localhost:11434")
    prompt = PromptTemplate(
        input_variables=["question", "graph", "papers"],
        template="""æ ¹æ“šä»¥ä¸‹å…©ç¨®ä¾†æºï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ä¸”å°ˆæ¥­çš„é†«å­¸å›ç­”ï¼š
        
        ä¾†æº1 (GraphRAG çŸ¥è­˜åœ–è­œ):
        {graph}

        ä¾†æº2 (PapersRAG æœ€æ–°é†«å­¸æ–‡ç»):
        {papers}

        å•é¡Œï¼š
        {question}
        
        è«‹åƒ…å¼•ç”¨ ä¾†æº2 æä¾›çš„ PMID èˆ‡æ®µè½ï¼Œä¸è¦è‡ªè¡Œç”Ÿæˆæˆ–æé€ åƒè€ƒæ–‡ç»ã€‚
        è«‹æ•´åˆä»¥ä¸Šè³‡æ–™ï¼Œç”Ÿæˆå°ˆæ¥­é†«å­¸å›ç­”ã€‚
        """
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    return chain.run(question=query, graph=graph_context, papers=papers_context)

# ========== Multi-Agent Managing System ==========
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

class SubQuery(BaseModel):
    subquery: str = Field(description="The rewritten sub-question")
    agent: str = Field(description="Which agent should handle this sub-question. Must be either GraphRAG or PapersRAG")

class QueryPlan(BaseModel):
    tasks: List[SubQuery]

def create_query_plan(query, llm_model):
    """
    Use LLM to break down the user's query into sub-queries and decide which agent should handle each.
    """
    llm = Ollama(model=llm_model, temperature=0, base_url="http://localhost:11434")
    parser = PydanticOutputParser(pydantic_object=QueryPlan)
    prompt = PromptTemplate(
        input_variables=["question"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
        template="""
ä½ æ˜¯ä¸€å€‹é†«å­¸å¤šä»£ç†ç³»çµ±çš„ä»»å‹™ç®¡ç†ä»£ç†ï¼Œè² è²¬å°‡è¤‡é›œçš„é†«å­¸å•é¡Œæ‹†è§£ç‚ºå­å•é¡Œï¼Œä¸¦æŒ‡æ´¾çµ¦ä¸åŒçš„å°ˆå®¶ä»£ç†ï¼š

- GraphRAG Agent: é©åˆæŸ¥è©¢çµæ§‹åŒ–ç”Ÿç‰©é†«å­¸çŸ¥è­˜åœ–è­œã€‚
- PapersRAG Agent: é©åˆæŸ¥è©¢æœ€æ–°é†«å­¸æ–‡ç»ã€‚

å°ä»¥ä¸‹å•é¡Œé€²è¡Œåˆ†æï¼Œç”¢ç”Ÿä¸€å€‹ JSON æ ¼å¼çš„ä»»å‹™è¨ˆç•«ï¼Œæ ¼å¼å¿…é ˆç‚ºï¼š
{format_instructions}

å•é¡Œï¼š
{question}
"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(question=query)
    return parser.parse(output)

# ========== Main Logic ==========
if submit and query:
    with st.spinner("æ­£åœ¨é€²è¡Œå¤šä»£ç†å”ä½œæŸ¥è©¢..."):
        try:
            # Step 1. Create task plan
            plan = create_query_plan(query, managing_model)
            st.info("ğŸ¤– Managing Agent å»ºç«‹çš„æŸ¥è©¢è¨ˆç•«ï¼š")
            st.json(plan.dict())

            graph_context = ""
            papers_context = ""

            # Step 2. Execute subtasks
            for task in plan.tasks:
                st.write(f"ğŸ”¹ è™•ç†å­å•é¡Œ: **{task.subquery}** â†’ æŒ‡æ´¾çµ¦ **{task.agent}**")

                if task.agent == "GraphRAG":
                    results = retriever.search(query_text=task.subquery)
                    if results:
                        parsed_records = []
                        for cypher_query, result_data in results:
                            if isinstance(result_data, list):
                                for record in result_data:
                                    content = getattr(record, "content", None)
                                    if not content:
                                        continue
                                    match = re.search(r"properties=\{(.+?)\}", content)
                                    if match:
                                        props = ast.literal_eval(f"{{{match.group(1)}}}")
                                        parsed_records.append(props)
                                    else:
                                        parsed_records.append({"result": content})
                        if parsed_records:
                            graph_context += f"\n### å­å•é¡Œ: {task.subquery}\n" + "\n".join([str(r) for r in parsed_records])
                            st.subheader("ğŸ“Š GraphRAG å­æŸ¥è©¢çµæœ")
                            st.dataframe(pd.DataFrame(parsed_records))
                        else:
                            st.warning(f"GraphRAG å°å­å•é¡Œ '{task.subquery}' æœªæ‰¾åˆ°çµæœ")

                elif task.agent == "PapersRAG":
                    # A) ç”±å­å•é¡Œå‹•æ…‹ç”¢ç”Ÿ must termsï¼ˆåªç”¨æ–¼éæ¿¾ç‰‡æ®µï¼Œä¸é‡çµ„ PubMed æŸ¥è©¢ï¼‰
                    must_terms = derive_core_terms(task.subquery)

                    # B) ç›´æ¥ç”¨åŸå§‹å­å•é¡Œä¸Ÿçµ¦ PubMed
                    raw_query = task.subquery
                    papers = retrieve_papers(raw_query, max_results=30)

                    if not papers:
                        st.warning(f"PapersRAG å°å­å•é¡Œ '{task.subquery}' æœªæ‰¾åˆ°ç›¸é—œè«–æ–‡")
                    else:
                        # C) åˆ‡å¡Š â†’ å…ˆåšä¸€æ¬¡ä¸»é¡Œè©éæ¿¾ï¼ˆå¯ä¿ç•™ä¹Ÿå¯æ‹¿æ‰ï¼Œå»ºè­°ä¿ç•™ä»¥é¿å…é›¢é¡Œï¼‰
                        all_chunks = []
                        for p in papers:
                            all_chunks.extend(build_chunks_from_entry(p))

                        filtered_chunks = filter_chunks_by_terms(all_chunks, must_terms)

                        if not filtered_chunks:
                            st.subheader("ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœ")
                            st.info("æª¢ç´¢åˆ°è«–æ–‡ï¼Œä½†å…¶å…§æ–‡ç‰‡æ®µæ²’æœ‰åŒ…å«ä¸»é¡Œé—œéµè©ï¼Œåƒ…åˆ—å‡ºè«–æ–‡ä»¥ä¾›äººå·¥æª¢è¦–ï¼š")
                            for p in papers[:10]:
                                st.markdown(f"**{p.get('title','(no title)')}** ([link]({p['pubmed_url']}))")
                            papers_context += (
                                f"\n### å­å•é¡Œ: {task.subquery}\n"
                                + "\n".join([json.dumps(p, ensure_ascii=False) for p in papers[:10]])
                                + "\n\n(æ³¨æ„ï¼šæ­¤å­å•é¡Œåœ¨å€™é¸ç‰‡æ®µä¸­æœªæ‰¾åˆ°åŒ…å«ä¸»é¡Œè©çš„å…§å®¹ã€‚)"
                            )
                        else:
                            # D) æª¢ç´¢èˆ‡é‡æ’
                            index = PapersMiniIndex(filtered_chunks)
                            hits = index.search(task.subquery, top_k_dense=12, top_k_bm25=12)
                            hits = rerank_hits(task.subquery, hits, top_k=8)

                            # E) æœ€çµ‚å†éæ¿¾ä¸€æ¬¡ï¼ˆä¿å®ˆåšæ³•ï¼Œç¢ºä¿é€é€² LLM çš„ç‰‡æ®µéƒ½å«ä¸»é¡Œè©ï¼‰
                            hits = filter_hits_by_terms(hits, must_terms)

                            if not hits:
                                st.subheader("ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœ")
                                st.info("å·²éæ¿¾é›¢é¡Œç‰‡æ®µï¼Œä½†æœ€çµ‚æ²’æœ‰ç¬¦åˆä¸»é¡Œè©çš„æ®µè½ã€‚ä»¥ä¸‹ç‚ºæª¢ç´¢åˆ°çš„è«–æ–‡æ¸…å–®ï¼š")
                                for p in papers[:10]:
                                    st.markdown(f"**{p.get('title','(no title)')}** ([link]({p['pubmed_url']}))")
                                papers_context += (
                                    f"\n### å­å•é¡Œ: {task.subquery}\n"
                                    + "\n".join([json.dumps(p, ensure_ascii=False) for p in papers[:10]])
                                    + "\n\n(æ³¨æ„ï¼šRAG ç‰‡æ®µç¶“ä¸»é¡Œè©éæ¿¾å¾Œç‚ºç©ºã€‚)"
                                )
                            else:
                                # F) çµ„è£å¸¶ç·¨è™Ÿå¼•æ–‡ context
                                cited_context = make_cited_context(hits)

                                # UI é¡¯ç¤º
                                st.subheader("ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœï¼ˆRAGï¼‰")
                                for i, h in enumerate(hits, 1):
                                    m = h["meta"]
                                    pmid = m.get("pmid", "NA")
                                    where = m.get("where", "")
                                    url = m.get("pubmed_url", "")
                                    title = m.get("title", "(no title)")
                                    score = h.get("rerank", h.get("score", 0.0))
                                    st.markdown(
                                        f"**[{i}] {title}** â€” *{where}*  \n"
                                        f"PMID: `{pmid}` ï½œ Score: `{score:.3f}` ï½œ [PubMed]({url})"
                                    )
                                    st.write(h["text"][:500] + ("..." if len(h["text"]) > 500 else ""))

                                # G) äº¤çµ¦æœ€çµ‚ç”Ÿæˆå™¨ä½¿ç”¨
                                papers_context += f"\n### å­å•é¡Œ: {task.subquery}\n" + cited_context

            # Step 3. Final synthesis
            final_answer = synthesize_answer(query, graph_context, papers_context, answer_model)
            st.subheader("ğŸ’¬ æœ€çµ‚å›ç­”")
            st.success(final_answer)

        except Exception as e:
            st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}") # è½‰ cypher æ»¿å®¹æ˜“è½‰éŒ¯çš„ï¼Œè¦æ˜¯è½‰éŒ¯å°±æœƒç›´æ¥åœæ‰ï¼Œè¦æ”¹ä¸€ä¸‹é‚è¼¯
