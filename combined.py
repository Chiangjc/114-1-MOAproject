import streamlit as st
from neo4j import GraphDatabase
from neo4j_graphrag.retrievers import Text2CypherRetriever
from neo4j_graphrag.llm import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import Ollama
import pandas as pd
import re
import ast
import os
import json
from Bio import Entrez
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import asyncio
import time  # â–¶ï¸ ç”¨æ–¼è¨ˆæ™‚
import hashlib
from datetime import datetime, timedelta
import chromadb

# ====================== å¯èª¿åƒæ•¸ ======================
TOP_N_PAPERS = 5  # â–¶ï¸ PapersRAG æœ€å¤šå¼•ç”¨çš„è«–æ–‡ç¯‡æ•¸ï¼ˆå¯è‡ªè¡Œæ”¹æˆ 10 ç­‰ï¼‰

# ========== Streamlit UI ==========
st.set_page_config(page_title="Medical RAG System", layout="wide")
st.title("ğŸ§  Multi-Agent Medical RAG System")
st.markdown("çµåˆ **GraphRAG (PrimeKG)** + **PubMed Papers RAG**ï¼Œç‚ºæ‚¨æä¾›é†«å­¸ç ”ç©¶ç­”æ¡ˆ")

query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„é†«å­¸å•é¡Œ")

managing_model = st.selectbox(
    "é¸æ“‡ Managing Agent æ¨¡å‹",
    ["llama3", "mistral", "qwen2:7b", "gpt-oss"]
)
cypher_model = st.selectbox(
    "é¸æ“‡ Cypher æŸ¥è©¢æ¨¡å‹",
    ["gpt-oss"]
)
answer_model = st.selectbox(
    "é¸æ“‡ å›ç­”ç”Ÿæˆæ¨¡å‹",
    ["llama3", "mistral", "gpt-oss"]
)

submit = st.button("æŸ¥è©¢")

# ========== Neo4j Initialization ==========
@st.cache_resource
def init_graph(cypher_model_name, answer_model_name):
    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))
    cypher_llm = OllamaLLM(model_name=cypher_model_name, model_params={"temperature": 0}, host="http://localhost:11434")
    answer_llm = Ollama(model=answer_model_name, temperature=0, base_url="http://localhost:11434")
    retriever = Text2CypherRetriever(driver=driver, llm=cypher_llm)
    return driver, cypher_llm, answer_llm, retriever

driver, cypher_llm, answer_llm, retriever = init_graph(cypher_model, answer_model)

def get_session():
    return driver.session(database="neo4j")

# ========== PubMed Utility Functions ==========
Entrez.email = "email@gmail.com"  # TODO: Replace with your email

def search_pubmed(keyword, max_results=20):
    handle = Entrez.esearch(db="pubmed", term=keyword, retmax=max_results, sort="relevance")
    record = Entrez.read(handle)
    return record["IdList"]

def get_pmc_id(pmid):
    handle = Entrez.elink(dbfrom="pubmed", db="pmc", id=pmid)
    record = Entrez.read(handle)
    try:
        return record[0]['LinkSetDb'][0]['Link'][0]['Id']
    except:
        return None

def get_title_from_pubmed(pmid):
    handle = Entrez.efetch(db="pubmed", id=pmid, rettype="medline", retmode="text")
    lines = handle.read().split("\n")
    title_lines = [line[6:].strip() for line in lines if line.startswith("TI  ") or line.startswith("      ")]
    return " ".join(title_lines) if title_lines else ""

def get_fulltext_from_europe_pmc(pmcid):
    url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/PMC{pmcid}/fullTextXML"
    response = requests.get(url)
    return response.text if response.status_code == 200 else None

def extract_fulltext_structure(xml_text):
    soup = BeautifulSoup(xml_text, "xml")
    title = soup.find("article-title")
    abstract = soup.find("abstract")
    body_paragraphs = [p.get_text().strip() for p in soup.find_all("p") if p.get_text().strip()]
    return {
        "title": title.get_text().strip() if title else "",
        "abstract": abstract.get_text(separator=" ", strip=True) if abstract else "",
        "body": body_paragraphs
    }

def retrieve_papers(keyword, max_results=20):
    pmids = search_pubmed(keyword, max_results=max_results)
    papers = []
    for pmid in pmids:
        pmc_id = get_pmc_id(pmid)
        title = get_title_from_pubmed(pmid)
        entry = {
            "pmid": pmid,
            "title": title,
            "pubmed_url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
        }
        if pmc_id:
            xml = get_fulltext_from_europe_pmc(pmc_id)
            if xml:
                entry["fulltext"] = extract_fulltext_structure(xml)
        papers.append(entry)
    return papers


# ç°¡å–®åœç”¨å­—ï¼Œå¯è‡ªè¡Œæ“´å……
_STOPWORDS = set("""
the a an and or of for to with without in on by from about as is are was were be being been into over under than then this that those these such using use used based among between
""".split())

def _clean_phrase(s: str) -> str:
    s = s.strip().strip('"').strip("'")
    s = re.sub(r"\s+", " ", s)
    return s

def derive_core_terms(user_query: str) -> list[str]:
    """
    å¾ä½¿ç”¨è€… query å‹•æ…‹æŠ½ä¸»é¡Œè©ï¼ˆç–¾ç—… / ä¸»é«”ï¼‰ï¼Œä½œç‚º must termsã€‚
    è¦å‰‡ï¼š
    - å…ˆæŠ“å¼•è™Ÿä¸­çš„ç‰‡èª
    - æ²’æœ‰å¼•è™Ÿå‰‡å–é•·åº¦ >= 3 çš„é—œéµè©ï¼Œå»åœç”¨å­—
    - è‡³å°‘ä¿ç•™ä¸€å€‹è©ï¼ˆæ•´å¥ï¼‰
    """
    q = user_query.strip()
    quoted = re.findall(r'"([^"]+)"|\'([^\']+)\'', q)
    phrases = [p[0] or p[1] for p in quoted]
    if phrases:
        core = [_clean_phrase(p) for p in phrases if _clean_phrase(p)]
    else:
        toks = [t for t in re.split(r"[^A-Za-z0-9\-]+", q) if t]
        toks = [t for t in toks if len(t) >= 3 and t.lower() not in _STOPWORDS]
        core = toks[:5] if toks else [q]
    # å»é‡ä¸¦ä¿ç•™åŸé †åº
    seen, core_unique = set(), []
    for t in core:
        tl = t.lower()
        if tl not in seen:
            seen.add(tl)
            core_unique.append(t)
    return core_unique

def contains_any(text: str, terms: list[str]) -> bool:
    t = text.lower()
    return any(term.lower() in t for term in terms)

# ========== PapersRAG RAG Helpers  ==========
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    _emb_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    _reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
except Exception:
    _emb_model = None
    _reranker = None

try:
    from rank_bm25 import BM25Okapi
except Exception:
    BM25Okapi = None

import unicodedata

def _normalize_ws(s: str) -> str:
    return " ".join(s.split())

def _chunk_text(text: str, max_chars: int = 1800, overlap: int = 200):
    text = _normalize_ws(text)
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks

def _split_sentences(text: str) -> list[str]:
    """
    ç”¨ç°¡å–®è¦å‰‡æŠŠæ®µè½åˆ‡æˆå¥å­ã€‚
    è‹±æ–‡ä¾æ“š ., ?, ! å¾ŒåŠ ç©ºç™½åˆ†å¥ï¼›ä¸­æ–‡ä¾æ“š ã€‚ï¼ï¼Ÿã€‚
    """
    import re
    text = text.stript()
    if not text:
        return []
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ!?\.])\s+', text)
    return [s.strip() for s in sentences if s.strip()]

def build_chunks_from_entry(entry: dict):
    chunks = []
    pmid = entry.get("pmid")
    title = entry.get("title", "")
    url = entry.get("pubmed_url", "")

    # æ‘˜è¦
    if entry.get("fulltext", {}).get("abstract"):
        for i, ch in enumerate(_split_sentences(entry["fulltext"]["abstract"])):
            chunks.append({
                "pmid": pmid, "title": title, "where": "abstract",
                "chunk_id": f"abs-s{i}", "text": ch, "pubmed_url": url
            })

    # æ­£æ–‡
    if entry.get("fulltext", {}).get("body"):
        for j, para in enumerate(entry["fulltext"]["body"]):
            for i, ch in enumerate(_split_sentences(para)):
                chunks.append({
                    "pmid": pmid, "title": title, "where": "body",
                    "chunk_id": f"p{j}-s{i}", "text": ch, "pubmed_url": url
                })

    # æ²’æœ‰å…¨æ–‡æ™‚ï¼Œè‡³å°‘ç”¨ title
    if not chunks and title.strip():
        chunks.append({
            "pmid": pmid, "title": title, "where": "title",
            "chunk_id": "t-0", "text": title.strip(), "pubmed_url": url
        })

    return chunks

def _tokenize_for_bm25(txt: str):
    txt = unicodedata.normalize("NFKC", txt.lower())
    cleaned = []
    for ch in txt:
        if ch.isalnum() or ch.isspace():
            cleaned.append(ch)
        else:
            if ord(ch) > 127 and not ch.isspace():
                cleaned.append(f" {ch} ")
    return " ".join("".join(cleaned).split()).split()

class PapersMiniIndex:
    """
    éæŒä¹…åŒ–çš„å°å‹ç´¢å¼•ã€‚
    - è‹¥è£äº† sentence-transformersï¼šç”¨å‘é‡ç›¸ä¼¼åº¦
    - è‹¥è£äº† rank-bm25ï¼šç”¨ BM25
    - å…©è€…éƒ½æœ‰ï¼šåšç°¡å–® hybridï¼ˆåˆ†æ•¸æ¨™æº–åŒ–å¾Œåˆä½µï¼‰
    - å…¨éƒ½æ²’æœ‰ï¼šç”¨é—œéµè©å‘½ä¸­åˆ†æ•¸
    """
    def __init__(self, chunks: list[dict]):
        self.docs = [c["text"] for c in chunks]
        self.metas = chunks
        self._dense = None
        self._bm25 = None

        if _emb_model is not None and self.docs:
            self._dense = _emb_model.encode(self.docs, show_progress_bar=False, normalize_embeddings=True)

        if BM25Okapi is not None and self.docs:
            self._bm25 = BM25Okapi([_tokenize_for_bm25(d) for d in self.docs])

    def search(self, query: str, top_k_dense=10, top_k_bm25=10):
        scores = {}

        # Dense ç›¸ä¼¼åº¦
        if self._dense is not None:
            import numpy as np
            q = _emb_model.encode([query], show_progress_bar=False, normalize_embeddings=True)[0]
            dense_scores = np.dot(self._dense, q)
            idxs = dense_scores.argsort()[-top_k_dense:][::-1]
            ds = dense_scores[idxs]
            if len(ds) > 1:
                ds_norm = (ds - ds.min()) / (ds.max() - ds.min() + 1e-9)
            else:
                ds_norm = ds
            for i, s in zip(idxs, ds_norm):
                scores[i] = max(scores.get(i, 0.0), float(s))

        # BM25 åˆ†æ•¸
        if self._bm25 is not None:
            import numpy as np
            bm25 = self._bm25.get_scores(_tokenize_for_bm25(query))
            idxs = bm25.argsort()[-top_k_bm25:][::-1]
            bs = bm25[idxs]
            if len(bs) > 1:
                bs_norm = (bs - bs.min()) / (bs.max() - bs.min() + 1e-9)
            else:
                bs_norm = bs
            for i, s in zip(idxs, bs_norm):
                scores[i] = max(scores.get(i, 0.0), float(s))

        # å¦‚æœå…©è€…éƒ½æ²’æœ‰ï¼Œå°±ç”¨é—œéµè©å‘½ä¸­æ•¸
        if not scores:
            q_terms = set(query.lower().split())
            for i, d in enumerate(self.docs):
                hit = sum(1 for t in q_terms if t in d.lower())
                if hit:
                    scores[i] = float(hit)

        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        hits = [{"text": self.docs[i], "meta": self.metas[i], "score": s} for i, s in ranked]
        return hits

def rerank_hits(query: str, hits: list[dict], top_k: int = 8):
    if not hits:
        return []
    if _reranker is None:
        return hits[:top_k]
    pairs = [(query, h["text"]) for h in hits[: max(32, top_k*2)]]
    scores = _reranker.predict(pairs)
    for h, s in zip(hits[:len(pairs)], scores):
        h["rerank"] = float(s)
    hits = sorted(hits[:len(pairs)], key=lambda x: x.get("rerank", 0.0), reverse=True)[:top_k]
    return hits

def make_cited_context(hits: list[dict]) -> str:
    blocks = []
    for i, h in enumerate(hits, 1):
        m = h["meta"]
        pmid = m.get("pmid", "NA")
        title = m.get("title", "")
        where = m.get("where", "")
        url = m.get("pubmed_url", "")
        txt = h["text"]
        blocks.append(
            f"[{i}] PMID {pmid} | {title} | {where}\n{txt}\nLink: {url}"
        )
    return "\n\n".join(blocks)

def filter_chunks_by_terms(chunks: list[dict], must_terms: list[str]) -> list[dict]:
    if not must_terms:
        return chunks
    out = []
    for c in chunks:
        if contains_any(c.get("text", ""), must_terms) or contains_any(c.get("title",""), must_terms):
            out.append(c)
    return out

def filter_hits_by_terms(hits: list[dict], must_terms: list[str]) -> list[dict]:
    if not must_terms:
        return hits
    out = []
    for h in hits:
        if contains_any(h.get("text",""), must_terms) or contains_any(h.get("meta",{}).get("title",""), must_terms):
            out.append(h)
    return out

# ========== Final Answer Synthesizer ==========
def synthesize_answer(query, graph_context, papers_context, model):
    llm = Ollama(model=model, temperature=0, base_url="http://localhost:11434")
    prompt = PromptTemplate(
        input_variables=["question", "graph", "papers"],
        template="""
ä½ æ˜¯ä¸€å€‹é†«å­¸åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ä¾†æºç”¢ç”Ÿå›ç­”ã€‚å‹™å¿…éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š

1. åƒ…èƒ½ä½¿ç”¨ä¾†æº1 (GraphRAG) å’Œä¾†æº2 (PapersRAG) æä¾›çš„å…§å®¹ï¼Œä¸è¦è‡ªè¡Œæ–°å¢ä»»ä½•é¡å¤–çŸ¥è­˜ã€‚
2. æ•´ç†ä¾†æºä¸­çš„æœ‰ç”¨è³‡è¨Šï¼Œé¿å…é‡è¤‡ï¼Œä½†è¦ç›¡å¯èƒ½æ¶µè“‹æ‰€æœ‰æ­£ç¢ºçš„ç´°ç¯€ã€‚
3. å¦‚æœå¼•ç”¨ ä¾†æº1ï¼Œè«‹æ¨™è¨»ã€Œ[GraphRAG]ã€ï¼Œä¸¦åªç”¨ç¯€é»èˆ‡é—œä¿‚è³‡è¨Šã€‚
4. å¦‚æœå¼•ç”¨ ä¾†æº2ï¼Œè«‹æ¨™è¨» PMIDï¼Œä¸¦ç›´æ¥å¼•ç”¨ä¾†æºä¸­å‡ºç¾çš„æ®µè½æ–‡å­—æˆ–æ•¸æ“šã€‚
   - æ ¼å¼ç¯„ä¾‹ï¼šæ ¹æ“š PMID: 123456 çš„ç ”ç©¶ï¼Œè—¥ç‰© X å¯æ”¹å–„æ°£å–˜æ§åˆ¶ã€‚
   - ä¸è¦ç”Ÿæˆæˆ–ä¿®æ”¹ä¾†æºä¸­æœªå‡ºç¾çš„æ–‡ç»ã€‚
5. è‹¥åŒæ™‚æœ‰å…©å€‹ä¾†æºï¼Œè«‹å…ˆæ•´ç† GraphRAG çš„çµæ§‹åŒ–çŸ¥è­˜ï¼Œå†æ•´ç† PapersRAG çš„ç ”ç©¶çµæœï¼Œæœ€å¾Œåšç°¡çŸ­ç¶œåˆã€‚
6. ä¿æŒå°ˆæ¥­ã€å®¢è§€ã€ç²¾ç°¡ï¼Œé¿å…èª‡å¤§æˆ–è‡†æ¸¬ã€‚

---

ä¾†æº1 (GraphRAG çŸ¥è­˜åœ–è­œ):
{graph}

ä¾†æº2 (PapersRAG æœ€æ–°é†«å­¸æ–‡ç»):
{papers}

å•é¡Œï¼š
{question}

è«‹ä¾è¦å‰‡ï¼Œæ•´ç†å‡ºæœ€çµ‚å›ç­”ã€‚
"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    return chain.run(question=query, graph=graph_context, papers=papers_context)

# ========== Multi-Agent Managing System ==========
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

class SubQuery(BaseModel):
    subquery: str = Field(description="The rewritten sub-question")
    agent: str = Field(description="Which agent should handle this sub-question. Must be either GraphRAG or PapersRAG")

class QueryPlan(BaseModel):
    tasks: List[SubQuery]

def create_query_plan(query, llm_model):
    llm = Ollama(model=llm_model, temperature=0, base_url="http://localhost:11434")
    parser = PydanticOutputParser(pydantic_object=QueryPlan)
    prompt = PromptTemplate(
        input_variables=["question"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
        template="""
ä½ æ˜¯ä¸€å€‹é†«å­¸å¤šä»£ç†ç³»çµ±çš„ä»»å‹™ç®¡ç†ä»£ç†ï¼Œè² è²¬å°‡è¤‡é›œçš„é†«å­¸å•é¡Œæ‹†è§£ç‚ºå­å•é¡Œï¼Œä¸¦æŒ‡æ´¾çµ¦ä¸åŒçš„å°ˆå®¶ä»£ç†ï¼š

- GraphRAG Agent: é©åˆæŸ¥è©¢çµæ§‹åŒ–ç”Ÿç‰©é†«å­¸çŸ¥è­˜åœ–è­œã€‚
- PapersRAG Agent: é©åˆæŸ¥è©¢æœ€æ–°é†«å­¸æ–‡ç»ï¼Œè«‹ç‰¹åˆ¥æ³¨æ„æŒ‡æ´¾çµ¦ä»–çš„å•é¡Œåªèƒ½æ˜¯è‹±æ–‡ï¼Œå¦‚æœæ˜¯ä¸­æ–‡æˆ–å…¶ä»–èªè¨€çš„è©±ï¼Œè«‹å…ˆç¿»è­¯æˆè‹±æ–‡å†æŒ‡æ´¾ã€‚

å°ä»¥ä¸‹å•é¡Œé€²è¡Œåˆ†æï¼Œç”¢ç”Ÿä¸€å€‹ JSON æ ¼å¼çš„ä»»å‹™è¨ˆç•«ï¼Œæ ¼å¼å¿…é ˆç‚ºï¼š
{format_instructions}

å•é¡Œï¼š
{question}
"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(question=query)
    return parser.parse(output)

# ===== Main Logic =====
if submit and query:
    overall_t0 = time.perf_counter()
    with st.spinner("æ­£åœ¨é€²è¡Œå¤šä»£ç†å”ä½œæŸ¥è©¢..."):
        try:
            # Step 1. Create task planï¼ˆè¨ˆæ™‚ï¼‰
            tm_t0 = time.perf_counter()
            plan = create_query_plan(query, managing_model)
            tm_t1 = time.perf_counter()

            st.info("ğŸ¤– Managing Agent å»ºç«‹çš„æŸ¥è©¢è¨ˆç•«ï¼š")
            st.json(plan.model_dump())
            st.caption(f"â±ï¸ Managing Agent ç”¨æ™‚ï¼š{tm_t1 - tm_t0:.2f} s")

            graph_context = ""  # â† åˆå§‹åŒ– GraphRAG ä¸Šä¸‹æ–‡
            papers_context = "" # â† åˆå§‹åŒ– PapersRAG ä¸Šä¸‹æ–‡

            # Step 2. Execute subtasks
            for task in plan.tasks:
                st.write(f"ğŸ”¹ è™•ç†å­å•é¡Œ: **{task.subquery}** â†’ æŒ‡æ´¾çµ¦ **{task.agent}**")

                if task.agent == "GraphRAG":
                    gr_t0 = time.perf_counter()
                    results = retriever.search(query_text=task.subquery)
                    if results:
                        st.subheader("ğŸ“„ Cypher æŸ¥è©¢èˆ‡çµæœ")
                        for idx, (cypher_query, result_data) in enumerate(results):
                            st.markdown(f"##### ğŸ” Result {idx + 1}")
                            st.code(cypher_query, language="cypher")

                            # å°‡çµæœæ•´ç†æˆ context_textï¼Œæ”¹æˆç´¯åŠ åˆ° graph_context
                            if isinstance(result_data, list):
                                parsed_records = []
                                for record in result_data:
                                    if not hasattr(record, 'content') or record.content is None:
                                        continue
                                    content = record.content

                                    # è§£æ properties
                                    match = re.search(r"properties=\{(.+?)\}", content)
                                    if match:
                                        properties_str = match.group(1)
                                        try:
                                            props = ast.literal_eval(f"{{{properties_str}}}")
                                            parsed_records.append(props)
                                        except:
                                            parsed_records.append({'result': content})
                                    else:
                                        try:
                                            content_dict = ast.literal_eval(content)
                                            if isinstance(content_dict, dict):
                                                parsed_records.append(content_dict)
                                            else:
                                                parsed_records.append({'result': content_dict})
                                        except:
                                            parsed_records.append({'result': content})

                                if parsed_records:
                                    df = pd.DataFrame(parsed_records)
                                    st.dataframe(df)
                                    graph_context += "\n".join([str(r) for r in parsed_records]) + "\n"

                            elif isinstance(result_data, dict):
                                st.json(result_data)
                                graph_context += str(result_data) + "\n"
                    # === Embedding-based æª¢ç´¢ ===
                    try:
                        import torch
                        from sentence_transformers import SentenceTransformer

                        # è‡ªå‹•åµæ¸¬å¯ç”¨ device
                        if torch.cuda.is_available():
                            device = "cuda"
                        elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
                            device = "mps"   # Apple Silicon
                        else:
                            device = "cpu"

                        st.caption(f"ğŸ” Embedding æª¢ç´¢è£ç½®ï¼š{device}")

                        embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device=device)
                        query_vec = embedding_model.encode(task.subquery).tolist()

                        with driver.session() as session:
                            result = session.run(
                                """
                                WITH $queryVec AS queryVec
                                CALL db.index.vector.queryNodes(
                                    'drug_embedding_index', 5, queryVec
                                ) YIELD node, score
                                RETURN node.node_index AS id, node.indication AS indication, score
                                ORDER BY score DESC
                                """,
                                queryVec=query_vec,
                            )
                            embed_records = [dict(record) for record in result]

                        if embed_records:
                            st.subheader("ğŸ“Š GraphRAG Embedding æŸ¥è©¢çµæœ")
                            df_embed = pd.DataFrame(embed_records)
                            st.dataframe(df_embed)
                            graph_context += "\n".join([str(r) for r in embed_records]) + "\n"

                    except Exception as e:
                        st.warning(f"âš ï¸ Embedding æª¢ç´¢å¤±æ•—: {e}")
                    gr_t1 = time.perf_counter()
                    st.caption(f"â±ï¸ GraphRAG å­ä»»å‹™ç”¨æ™‚ï¼š{gr_t1 - gr_t0:.2f} s")

                elif task.agent == "PapersRAG":
                    pr_t0 = time.perf_counter()

                    # A) must termsï¼ˆåƒ…ç”¨æ–¼éæ¿¾ï¼‰
                    must_terms = derive_core_terms(task.subquery)

                    # B) PubMed æª¢ç´¢
                    t_pubmed_0 = time.perf_counter()
                    raw_query = task.subquery
                    papers = retrieve_papers(raw_query, max_results=30)
                    t_pubmed_1 = time.perf_counter()
                    st.caption(f"â±ï¸ PubMed æª¢ç´¢ï¼š{t_pubmed_1 - t_pubmed_0:.2f} sï¼Œå–å¾— {len(papers)} ç¯‡")

                    if not papers:
                        st.warning(f"PapersRAG å°å­å•é¡Œ '{task.subquery}' æœªæ‰¾åˆ°ç›¸é—œè«–æ–‡")
                    else:
                        # C) åˆ‡å¡Šèˆ‡åˆéæ¿¾
                        t_chunk_0 = time.perf_counter()
                        all_chunks = []
                        for p in papers:
                            all_chunks.extend(build_chunks_from_entry(p))
                        filtered_chunks = filter_chunks_by_terms(all_chunks, must_terms)
                        t_chunk_1 = time.perf_counter()
                        st.caption(
                            f"â±ï¸ åˆ‡å¡Š + åˆæ­¥éæ¿¾ï¼š{t_chunk_1 - t_chunk_0:.2f} sï¼Œchunksï¼š{len(all_chunks)} â†’ {len(filtered_chunks)}"
                        )

                        if not filtered_chunks:
                            st.subheader("ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœ")
                            st.info("æª¢ç´¢åˆ°è«–æ–‡ï¼Œä½†å…¶å…§æ–‡ç‰‡æ®µæ²’æœ‰åŒ…å«ä¸»é¡Œé—œéµè©ï¼Œåƒ…åˆ—å‡ºè«–æ–‡ä»¥ä¾›äººå·¥æª¢è¦–ï¼š")
                            for p in papers[:10]:
                                st.markdown(f"**{p.get('title','(no title)')}** ([link]({p['pubmed_url']}))")
                            papers_context += (
                                f"\n### å­å•é¡Œ: {task.subquery}\n"
                                + "\n".join([json.dumps(p, ensure_ascii=False) for p in papers[:10]])
                                + "\n\n(æ³¨æ„ï¼šæ­¤å­å•é¡Œåœ¨å€™é¸ç‰‡æ®µä¸­æœªæ‰¾åˆ°åŒ…å«ä¸»é¡Œè©çš„å…§å®¹ã€‚)"
                            )
                        else:
                            # D) æª¢ç´¢
                            t_search_0 = time.perf_counter()
                            index = PapersMiniIndex(filtered_chunks)
                            hits = index.search(task.subquery, top_k_dense=12, top_k_bm25=12)
                            t_search_1 = time.perf_counter()
                            st.caption(f"â±ï¸ Hybrid æª¢ç´¢ï¼š{t_search_1 - t_search_0:.2f} sï¼Œå‘½ä¸­ {len(hits)} å€‹ç‰‡æ®µ")

                            # E) ç²¾æ’
                            t_rerank_0 = time.perf_counter()
                            hits = rerank_hits(task.subquery, hits, top_k=64)  # å…ˆä¿ç•™è¼ƒå¤šï¼Œæ–¹ä¾¿é¸å‰ N ç¯‡
                            t_rerank_1 = time.perf_counter()
                            st.caption(f"â±ï¸ ç²¾æ’ï¼ˆCrossEncoder æˆ– Top-Kï¼‰ï¼š{t_rerank_1 - t_rerank_0:.2f} sï¼Œä¿ç•™ {len(hits)} ç‰‡æ®µ")

                            # F) çµ‚éæ¿¾ï¼ˆç¢ºä¿å«ä¸»é¡Œè©ï¼‰
                            t_filter_0 = time.perf_counter()
                            hits = filter_hits_by_terms(hits, must_terms)
                            t_filter_1 = time.perf_counter()
                            st.caption(f"â±ï¸ å‘½ä¸­å¾Œéæ¿¾ï¼š{t_filter_1 - t_filter_0:.2f} sï¼Œå‰© {len(hits)} ç‰‡æ®µ")

                            if not hits:
                                st.subheader("ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœ")
                                st.info("å·²éæ¿¾é›¢é¡Œç‰‡æ®µï¼Œä½†æœ€çµ‚æ²’æœ‰ç¬¦åˆä¸»é¡Œè©çš„æ®µè½ã€‚ä»¥ä¸‹ç‚ºæª¢ç´¢åˆ°çš„è«–æ–‡æ¸…å–®ï¼š")
                                for p in papers[:10]:
                                    st.markdown(f"**{p.get('title','(no title)')}** ([link]({p['pubmed_url']}))")
                                papers_context += (
                                    f"\n### å­å•é¡Œ: {task.subquery}\n"
                                    + "\n".join([json.dumps(p, ensure_ascii=False) for p in papers[:10]])
                                    + "\n\n(æ³¨æ„ï¼šRAG ç‰‡æ®µç¶“ä¸»é¡Œè©éæ¿¾å¾Œç‚ºç©ºã€‚)"
                                )
                            else:
                                # G) åªå–å‰ N ç¯‡è«–æ–‡ï¼ˆä¾ç‰‡æ®µæ’åºæŠ½å–å”¯ä¸€ PMIDï¼‰
                                t_pick_0 = time.perf_counter()
                                ordered_pmids = []
                                for h in hits:
                                    pmid = h["meta"].get("pmid")
                                    if pmid and pmid not in ordered_pmids:
                                        ordered_pmids.append(pmid)
                                    if len(ordered_pmids) >= int(TOP_N_PAPERS):
                                        break
                                selected_set = set(ordered_pmids)
                                selected_hits = [h for h in hits if h["meta"].get("pmid") in selected_set]
                                t_pick_1 = time.perf_counter()
                                st.caption(
                                    f"â±ï¸ æ“·å–å‰ {int(TOP_N_PAPERS)} ç¯‡ï¼š{t_pick_1 - t_pick_0:.2f} sï¼Œæœ€çµ‚ç‰‡æ®µæ•¸ {len(selected_hits)}ï¼Œè«–æ–‡æ•¸ {len(selected_set)}"
                                )

                                # H) çµ„è£å¼•æ–‡ context
                                t_ctx_0 = time.perf_counter()
                                cited_context = make_cited_context(selected_hits)
                                t_ctx_1 = time.perf_counter()
                                st.caption(f"â±ï¸ å¼•æ–‡çµ„è£ï¼š{t_ctx_1 - t_ctx_0:.2f} s")

                                # UI é¡¯ç¤º
                                st.subheader(f"ğŸ“„ PapersRAG å­æŸ¥è©¢çµæœï¼ˆå–å‰ {int(TOP_N_PAPERS)} ç¯‡ï¼‰")
                                for i, h in enumerate(selected_hits, 1):
                                    m = h["meta"]
                                    pmid = m.get("pmid", "NA")
                                    where = m.get("where", "")
                                    url = m.get("pubmed_url", "")
                                    title = m.get("title", "(no title)")
                                    score = h.get("rerank", h.get("score", 0.0))
                                    st.markdown(
                                        f"**[{i}] {title}** â€” *{where}*  \n"
                                        f"PMID: `{pmid}` ï½œ Score: `{score:.3f}` ï½œ [PubMed]({url})"
                                    )
                                    st.write(h["text"][:500] + ("..." if len(h["text"]) > 500 else ""))

                                # I) äº¤çµ¦æœ€çµ‚ç”Ÿæˆå™¨ä½¿ç”¨
                                papers_context += f"\n### å­å•é¡Œ: {task.subquery}\n" + cited_context

                    pr_t1 = time.perf_counter()
                    st.caption(f"â±ï¸ PapersRAG å­ä»»å‹™ç”¨æ™‚ï¼š{pr_t1 - pr_t0:.2f} s")

            # Step 3. Final synthesisï¼ˆè¨ˆæ™‚ï¼‰
            t_synth_0 = time.perf_counter()
            final_answer = synthesize_answer(query, graph_context, papers_context, answer_model)
            t_synth_1 = time.perf_counter()
            st.subheader("ğŸ’¬ æœ€çµ‚å›ç­”")
            st.success(final_answer)
            st.caption(f"â±ï¸ æœ€çµ‚ç­”æ¡ˆç”Ÿæˆï¼š{t_synth_1 - t_synth_0:.2f} s")

            overall_t1 = time.perf_counter()
            st.caption(f"â±ï¸ ç¸½ç”¨æ™‚ï¼š{overall_t1 - overall_t0:.2f} s")

        except Exception as e:
            st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")  # è½‰ Cypher æ»¿å®¹æ˜“è½‰éŒ¯çš„ï¼Œè¦æ˜¯è½‰éŒ¯å°±æœƒç›´æ¥åœæ‰ï¼Œè¦æ”¹ä¸€ä¸‹é‚è¼¯